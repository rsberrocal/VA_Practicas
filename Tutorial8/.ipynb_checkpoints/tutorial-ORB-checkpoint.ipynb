{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Censure and ORB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "## Descriptors extraction for object detection based on ORB\n",
    "==============================================================================================\n",
    "\n",
    "In this tutorial we focuss on the following main topics:\n",
    "\n",
    "- 1) Image Feature detector by CENSURE\n",
    "- 2) Image Feature descriptors by ORB\n",
    "- 3) Image matching (recognition by correspondence) based on feature extraction (ORB)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CENSURE\n",
    "\n",
    "What is CENSURE doing? \n",
    "\n",
    "Let's read a face image. Where are the image features detected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage import transform\n",
    "from skimage.feature import CENSURE\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "\n",
    "img_orig=io.imread('images/angelina.jpg')\n",
    "print(img_orig.shape)\n",
    "plt.imshow(img_orig)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Angelina')\n",
    "plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the parameters of the CENSURE detector. \n",
    "\n",
    "How does the CENSURE depend on the type of the image?\n",
    "\n",
    "How does the CENSURE depend on the scale, rotation, and change of contrast of the image?\n",
    "\n",
    "Check the CENSURE detectors of facial images vs nature images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "img_gr = rgb2gray(img_orig)\n",
    "\n",
    "detector = CENSURE()\n",
    "detector.detect(img_gr)\n",
    "print('I found ', len(detector.keypoints), 'keypoints')\n",
    "\n",
    "plt.imshow(img_orig, cmap=plt.cm.gray)\n",
    "plt.scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "plt.title(\"Keypoints in Angelina's Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the parameters of the function CENSURE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "detector = CENSURE(non_max_threshold=0.05)\n",
    "detector.detect(img_gr)\n",
    "print('I found ', len(detector.keypoints), 'keypoints')\n",
    "\n",
    "plt.imshow(img_orig, cmap=plt.cm.gray)\n",
    "plt.scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "plt.title(\"Angelina's Image\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Which is the effect of changing the parameter 'non_max_threshold'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "detector = CENSURE(line_threshold=50)\n",
    "detector.detect(img_gr)\n",
    "print('I found ', len(detector.keypoints), 'keypoints')\n",
    "\n",
    "plt.imshow(img_orig, cmap=plt.cm.gray)\n",
    "plt.scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "plt.title(\"Angelina's Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Which is the effect of changing the parameter 'line_threshold'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Make more changes of parameters and discuss the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Censure keypoints after rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tform = transform.AffineTransform(scale=(1, 1), rotation=0.15)\n",
    "\n",
    "img_warp = transform.warp(img_orig, tform,  mode='constant', cval=1)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_orig)\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(rgb2gray(img_warp))\n",
    "\n",
    "ax[1].imshow(img_warp)\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Censure keypoints after rescaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.transform import rescale\n",
    "\n",
    "img_small = rescale(img_gr,.5)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_gr, cmap='gray')\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(img_small)\n",
    "\n",
    "ax[1].imshow(img_small, cmap='gray')\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Censure keypoints after change of contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(img_gr.min(), img_gr.max()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "img_dark = rescale_intensity(img_gr, in_range=(0, 0.7))\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_gr, cmap='gray')\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(img_dark)\n",
    "\n",
    "ax[1].imshow(img_dark, cmap='gray')\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Censure keypoints in a nature image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_orig=io.imread('images/nature3.jpeg')\n",
    "print(img_orig.shape)\n",
    "plt.imshow(img_orig)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Nature Image')\n",
    "plt.show()\n",
    "\n",
    "img_gr=rgb2gray(img_orig)\n",
    "detector = CENSURE()\n",
    "detector.detect(img_gr)\n",
    "print('I found ', len(detector.keypoints), 'keypoints')\n",
    "\n",
    "plt.imshow(img_orig, cmap=plt.cm.gray)\n",
    "plt.scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "plt.title(\"Keypoints in Nature Image\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the parameters of the function CENSURE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detector = CENSURE(non_max_threshold=0.05)\n",
    "detector.detect(img_gr)\n",
    "print('I found ', len(detector.keypoints), 'keypoints')\n",
    "\n",
    "plt.imshow(img_orig, cmap=plt.cm.gray)\n",
    "plt.scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "plt.title(\"Nature Image\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "detector = CENSURE(line_threshold=50)\n",
    "detector.detect(img_gr)\n",
    "print('I found ', len(detector.keypoints), 'keypoints')\n",
    "\n",
    "plt.imshow(img_orig, cmap=plt.cm.gray)\n",
    "plt.scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "plt.title(\"Nature Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Make more changes of parameters and discuss the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Censure keypoints after rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tform = transform.AffineTransform(scale=(1, 1), rotation=0.15)\n",
    "\n",
    "img_warp = transform.warp(img_orig, tform,  mode='constant', cval=1)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_orig)\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(rgb2gray(img_warp))\n",
    "\n",
    "ax[1].imshow(img_warp)\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Censure keypoints after rescaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.transform import rescale\n",
    "\n",
    "img_small = rescale(img_gr,.5)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_gr, cmap='gray')\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(img_small)\n",
    "\n",
    "ax[1].imshow(img_small, cmap='gray')\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Censure keypoints after change of contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "img_dark = rescale_intensity(img_gr, in_range=(0, 0.7))\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_gr, cmap='gray')\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(img_dark)\n",
    "\n",
    "ax[1].imshow(img_dark, cmap='gray')\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Discuss what you observe and write your conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ORB feature detector and binary descriptor\n",
    "\n",
    "Let us consider the problem of feature extraction that contains two subproblems: \n",
    "- feature location, \n",
    "- image feature description.\n",
    "\n",
    "Let us apply ORB using the `ORB` function within the module `skimage.feature`.\n",
    "                             \n",
    "See an example in [ORB example](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_orb.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is ORB doing? \n",
    "\n",
    "Let's run the ORB detector on the face image. Where are the image feature detected?\n",
    "\n",
    "Explore the parameters of the ORB detector. \n",
    "\n",
    "How does the ORB depend on the type of the image?\n",
    "\n",
    "How does the ORB depend on the scale, rotation, and change of contrast of the image?\n",
    "\n",
    "Check the ORB detectors of facial images vs nature images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import ORB\n",
    "\n",
    "img_orig=io.imread('images/angelina.jpg')\n",
    "print(img_orig.shape)\n",
    "plt.imshow(img_orig)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Angelina')\n",
    "plt.show()\n",
    "\n",
    "img_gr = rgb2gray(img_orig)\n",
    "\n",
    "detector = ORB()\n",
    "detector.detect(img_gr)\n",
    "print('I found ', len(detector.keypoints), 'keypoints')\n",
    "\n",
    "plt.imshow(img_orig, cmap=plt.cm.gray)\n",
    "plt.scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "plt.title(\"Keypoints in Angelina's Image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detector1 = ORB()\n",
    "\n",
    "detector2 = ORB(n_keypoints=200)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector1.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_orig, cmap='gray')\n",
    "ax[0].scatter(detector1.keypoints[:, 1], detector1.keypoints[:, 0],\n",
    "              2 ** detector1.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector2.detect(img_gr)\n",
    "\n",
    "ax[1].imshow(img_orig, cmap='gray')\n",
    "ax[1].scatter(detector2.keypoints[:, 1], detector2.keypoints[:, 0],\n",
    "              2 ** detector2.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Change of parameter n_keypoints=200')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detector1 = ORB()\n",
    "detector2 = ORB(harris_k=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector1.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_orig, cmap='gray')\n",
    "ax[0].scatter(detector1.keypoints[:, 1], detector1.keypoints[:, 0],\n",
    "              2 ** detector1.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector2.detect(img_gr)\n",
    "\n",
    "ax[1].imshow(img_orig, cmap='gray')\n",
    "ax[1].scatter(detector2.keypoints[:, 1], detector2.keypoints[:, 0],\n",
    "              2 ** detector2.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Change of parameter harris_k=0')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Make more changes of parameters and discuss the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute ORB keypoints after rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tform = transform.AffineTransform(scale=(1, 1), rotation=0.15)\n",
    "\n",
    "img_warp = transform.warp(img_orig, tform,  mode='constant', cval=1)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_orig)\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(rgb2gray(img_warp))\n",
    "\n",
    "ax[1].imshow(img_warp)\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Do you think the result is invariant to rotation? Write your conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_small = rescale(img_gr,.5)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_gr, cmap='gray')\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(img_small)\n",
    "\n",
    "ax[1].imshow(img_small, cmap='gray')\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_dark = rescale_intensity(img_gr, in_range=(0, 0.7))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_gr, cmap='gray')\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(img_dark)\n",
    "\n",
    "ax[1].imshow(img_dark, cmap='gray')\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Discuss the results with your classmates and write your conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the nature image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_orig=io.imread('images/nature3.jpeg')\n",
    "print(img_orig.shape)\n",
    "plt.imshow(img_orig)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Nature')\n",
    "plt.show()\n",
    "\n",
    "img_gr=rgb2gray(img_orig)\n",
    "detector = ORB()\n",
    "detector.detect(img_gr)\n",
    "print('I found ', len(detector.keypoints), 'keypoints')\n",
    "\n",
    "plt.imshow(img_orig, cmap=plt.cm.gray)\n",
    "plt.scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "plt.title(\"Keypoints in Nature Image\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Make changes of parameters and discuss the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tform = transform.AffineTransform(scale=(1, 1), rotation=0.15)\n",
    "\n",
    "img_warp = transform.warp(img_orig, tform,  mode='constant', cval=1)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_orig)\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(rgb2gray(img_warp))\n",
    "\n",
    "ax[1].imshow(img_warp)\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_small = rescale(img_gr,.5)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_gr, cmap='gray')\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(img_small)\n",
    "\n",
    "ax[1].imshow(img_small, cmap='gray')\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_dark = rescale_intensity(img_gr, in_range=(0, 0.7))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "detector.detect(img_gr)\n",
    "\n",
    "ax[0].imshow(img_gr, cmap='gray')\n",
    "ax[0].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "detector.detect(img_dark)\n",
    "\n",
    "ax[1].imshow(img_dark, cmap='gray')\n",
    "ax[1].scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax[1].set_title('Transformed Image')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Observe the results and write your conclusions about the invariance to rotation, scale and change of contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using ORB feature detector to match transformed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import match_descriptors\n",
    "\n",
    "img = io.imread('./images/Building.jpg')\n",
    "img1 = rgb2gray(img)\n",
    "\n",
    "img2 = transform.rotate(img1, 180)\n",
    "\n",
    "descriptor_extractor = ORB(n_keypoints=200)\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img1)\n",
    "\n",
    "keypoints1 = descriptor_extractor.keypoints\n",
    "descriptors1 = descriptor_extractor.descriptors\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img2)\n",
    "\n",
    "keypoints2 = descriptor_extractor.keypoints\n",
    "descriptors2 = descriptor_extractor.descriptors\n",
    "\n",
    "matches12 = match_descriptors(descriptors1, descriptors2, cross_check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import plot_matches\n",
    "\n",
    "# Visualize the results:\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches12)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Original Image vs. Transformed Image\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Analyze and comment the observed result. Do you think it is good enough? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature  import match_descriptors, plot_matches\n",
    "\n",
    "# Let's do another transformation\n",
    "tform = transform.AffineTransform(scale=(1.3, 1.1), rotation=0.5,\n",
    "                                  translation=(0, -200))\n",
    "img3 = transform.warp(img1, tform)\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img3)\n",
    "\n",
    "keypoints3 = descriptor_extractor.keypoints\n",
    "descriptors3 = descriptor_extractor.descriptors\n",
    "\n",
    "matches13 = match_descriptors(descriptors1, descriptors3, cross_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the results:\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "plot_matches(ax, img1, img3, keypoints1, keypoints3, matches13)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Original Image vs. Transformed Image\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Analyze and comment the observed result. Do you think it is good enough? Are the matches all correct?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img3 = rescale(img1,.5)\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img3)\n",
    "\n",
    "keypoints3 = descriptor_extractor.keypoints\n",
    "descriptors3 = descriptor_extractor.descriptors\n",
    "\n",
    "matches13 = match_descriptors(descriptors1, descriptors3, cross_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the results:\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "plot_matches(ax, img1, img3, keypoints1, keypoints3, matches13)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Original Image vs. Transformed Image\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Analyze and comment the observed result. Do you think it is good enough? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img3 = rescale_intensity(img1, in_range=(0, 0.7))\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img3)\n",
    "\n",
    "keypoints3 = descriptor_extractor.keypoints\n",
    "descriptors3 = descriptor_extractor.descriptors\n",
    "\n",
    "matches13 = match_descriptors(descriptors1, descriptors3, cross_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the results:\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "plot_matches(ax, img1, img3, keypoints1, keypoints3, matches13)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Original Image vs. Transformed Image\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Analyze and comment the observed result. Do you think it is good enough? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to match ORB detectors between facial and nature image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img1=io.imread('images/angelina.jpg')\n",
    "img1 = rgb2gray(img1)\n",
    "\n",
    "img2=io.imread('images/nature3.jpeg')\n",
    "img2 = rgb2gray(img2)\n",
    "\n",
    "\n",
    "\n",
    "descriptor_extractor = ORB(n_keypoints=200)\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img1)\n",
    "\n",
    "keypoints1 = descriptor_extractor.keypoints\n",
    "descriptors1 = descriptor_extractor.descriptors\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img2)\n",
    "\n",
    "keypoints2 = descriptor_extractor.keypoints\n",
    "descriptors2 = descriptor_extractor.descriptors\n",
    "\n",
    "matches12 = match_descriptors(descriptors1, descriptors2, cross_check=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the results:\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches12)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Original Image vs. Transformed Image\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Exercise:** Analyze and comment the observed result. Do you think it is good enough? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
